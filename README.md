
# Time Series Forecasting Project

This repository contains a collection of Jupyter notebooks for
performing time series forecasting. The project demonstrates various
techniques for analyzing and predicting data that is collected over
time, with a specific focus on financial market data.

## Files

-   **forecasting.ipynb**: This notebook serves as a foundational guide
    to time series analysis. It covers essential steps such as data
    loading, exploratory data analysis (EDA), visualizing trends and
    seasonality, and checking for stationarity using the Augmented
    Dickey-Fuller (ADF) test. It also includes a comparison of ARIMA and
    SARIMA models.
-   **MNC_forecasting.ipynb**: This notebook applies time series
    forecasting to real-world financial data. It focuses on predicting
    the stock prices of a multinational corporation (MNC) using the
    yfinance library to fetch data and the ARIMA model to generate a
    forecast.

## Technologies

-   **Python**: The core programming language used for the project.
-   **Pandas**: A data manipulation and analysis library, used
    extensively for handling time series data.
-   **Matplotlib**: A plotting library used for visualizing data,
    trends, and model results.
-   **Statsmodels**: A powerful library for statistical modeling, which
    includes the ARIMA and SARIMA models for time series forecasting.
-   **yfinance**: A library that provides a simple way to download
    historical market data from Yahoo! Finance.

## Project Highlights

-   Developed and deployed a time series forecasting model in Python to
    predict monthly sales for a multinational corporation.
-   Utilized the Prophet library to analyze historical sales data,
    providing key business insights for strategic planning and resource
    allocation.
-   The model's accurate sales projections directly supported
    data-driven decision-making, which is crucial for supply chain and
    inventory optimization.
-   Demonstrated strong proficiency in the end-to-end data science
    pipeline, including data cleaning, in-depth analysis, and model
    evaluation.

    
